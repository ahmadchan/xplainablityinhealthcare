{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NER Code to return medical entities from Unstructured data"
      ],
      "metadata": {
        "id": "BEI0NrY4qSu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#installing required libraries\n",
        "!pip install -q -U spacy\n",
        "!pip install -q scispacy\n",
        "!pip install -q https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_sm-0.5.1.tar.gz"
      ],
      "metadata": {
        "id": "v2R-3pH8qScM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Function 'fetchEntity' to accept text and return the list of entities extracted from the text\n",
        "import scispacy\n",
        "import spacy\n",
        "import en_core_sci_sm\n",
        "def fetchEntity(unstructuredText=\"no input\"):\n",
        "  if unstructuredText==\"no input\":\n",
        "    print(\"No text was passed as input. You need to pass patient related text during function call.\")\n",
        "  else:\n",
        "    nlp_sm = en_core_sci_sm.load()\n",
        "    doc = nlp_sm(unstructuredText)\n",
        "    entityList=list(doc.ents)\n",
        "    #print(\"entityList is as follows: \\n\", entityList)\n",
        "    return entityList"
      ],
      "metadata": {
        "id": "k3-9m7eMudJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Codisep_Knowledge_Graph"
      ],
      "metadata": {
        "id": "t1VkBzJENQ52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install word2number"
      ],
      "metadata": {
        "id": "XKUWXjtXXfI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from word2number import w2n\n",
        "import inflect"
      ],
      "metadata": {
        "id": "xyff_mtGXjN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_duplicates_and_empty(lst):\n",
        "    return list(set(filter(None, lst)))\n",
        "\n",
        "# Example usage\n",
        "input_list = [\"apple\", \"banana\", \"apple\", \"\", \"orange\", \"banana\", \"\"]\n",
        "result = remove_duplicates_and_empty(input_list)\n",
        "print(result)  # Output: [\"apple\", \"banana\", \"orange\"]\n"
      ],
      "metadata": {
        "id": "zsxOkGHkQDds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def remove_special_characters_and_digits(input_string):\n",
        "    # Use regular expression to remove non-space characters that are not letters\n",
        "    cleaned_string = re.sub(r'[^a-zA-Z\\s]', '', input_string)\n",
        "    return cleaned_string\n",
        "\n",
        "# Example usage\n",
        "input_string = \"Hello! 123 This is a test string with special characters.\"\n",
        "cleaned_result = remove_special_characters_and_digits(input_string)\n",
        "print(cleaned_result)  # Output: \"Hello  This is a test string with special characters\"\n"
      ],
      "metadata": {
        "id": "GcjoVz6bUkF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def keep_only_english_characters(input_string):\n",
        "    # Remove numbers and special characters\n",
        "    cleaned_string = re.sub(r'[^a-zA-Z]', '', input_string)\n",
        "\n",
        "    # Remove non-English characters\n",
        "    cleaned_string = re.sub(r'[^\\x00-\\x7F]', '', cleaned_string)\n",
        "\n",
        "    return cleaned_string.lower()"
      ],
      "metadata": {
        "id": "Q9G9w8tDZNle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# Specify the path to the TSV file\n",
        "tsv_file_path = '/content/codisep_data_sorted_final.tsv'\n",
        "\n",
        "\n",
        "#Getting required items\n",
        "patient_ids=[]\n",
        "medical_codes=[]\n",
        "medical_codes_description=[]\n",
        "medical_concepts=[]\n",
        "medical_synonyms=[]\n",
        "medical_definitions=[]\n",
        "\n",
        "\n",
        "# Open the TSV file using a context manager\n",
        "with open(tsv_file_path, 'r', newline='', encoding='utf-8') as tsv_file:\n",
        "    # Create a CSV reader object with tab as the delimiter\n",
        "    tsv_reader = csv.reader(tsv_file, delimiter='\\t')\n",
        "\n",
        "    # Loop through the rows in the TSV file\n",
        "    for row in tsv_reader:\n",
        "        # Each row is a list of values, where each value corresponds to a column\n",
        "        # Process the values in the row as needed\n",
        "        #print(row[2])\n",
        "        patient_ids.append(row[0])\n",
        "        medical_codes.append(row[1])\n",
        "        medical_codes_description.append(row[2].lower())\n",
        "        medical_concepts.append(row[3].lower())\n",
        "        input_string = row[4]\n",
        "        split_parts = input_string.split(\"@@@@@\")\n",
        "        #print(split_parts)  # Output: ['username', 'example.com']\n",
        "        for i in split_parts:\n",
        "          medical_definitions.append(i.lower())\n",
        "        input_string = row[5]\n",
        "        split_parts = input_string.split(\"@@@@@\")\n",
        "        for i in split_parts:\n",
        "          medical_synonyms.append(i.lower())\n",
        "\n"
      ],
      "metadata": {
        "id": "ZbyJV-qMNXRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patient_ids=remove_duplicates_and_empty(patient_ids)\n",
        "medical_codes=remove_duplicates_and_empty(medical_codes)\n",
        "medical_codes_description=remove_duplicates_and_empty(medical_codes_description)\n",
        "medical_concepts=remove_duplicates_and_empty(medical_concepts)\n",
        "medical_definitions=remove_duplicates_and_empty(medical_definitions)\n",
        "medical_synonyms=remove_duplicates_and_empty(medical_synonyms)\n",
        "\n",
        "cypher_query_codisep=[]\n",
        "max_lenght=[]\n",
        "max_lenght.append(patient_ids)\n",
        "max_lenght.append(medical_codes)\n",
        "max_lenght.append(medical_codes_description)\n",
        "max_lenght.append(medical_concepts)\n",
        "max_lenght.append(medical_definitions)\n",
        "max_lenght.append(medical_synonyms)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "len(medical_synonyms)"
      ],
      "metadata": {
        "id": "y5HL5yO-QZ55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating the Knowledge Graph Nodes"
      ],
      "metadata": {
        "id": "mccK6jv3Ttiv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Patient Nodes"
      ],
      "metadata": {
        "id": "2dprV334Y1ql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n=0\n",
        "for i in patient_ids:\n",
        "  n=n+1\n",
        "  p = inflect.engine()\n",
        "  numeric_value = n\n",
        "  word = p.number_to_words(numeric_value)\n",
        "  cypher_query_codisep.append(\"(\"+i+\":patient { name: \"+'\"'+\"Patient \"+word+'\"'+\" }),\")"
      ],
      "metadata": {
        "id": "BMbOQ9gYYxba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Medical Code Nodes"
      ],
      "metadata": {
        "id": "QcR4bVieY6V6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in medical_codes:\n",
        "  cypher_query_codisep.append(\"(\"+i.lower()+\":medical_code { name: \"+'\"'+i+'\"'+\" }),\")"
      ],
      "metadata": {
        "id": "1lZRGgR-ZmBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Medical Code Descriptions"
      ],
      "metadata": {
        "id": "PBpSUZ1dZ9qc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in medical_codes_description:\n",
        "  cypher_query_codisep.append(\"(\"+keep_only_english_characters(i.lower())+\":medical_code_description { name: \"+'\"'+i+'\"'+\" }),\")"
      ],
      "metadata": {
        "id": "MnUNdNI9Z9Ve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Medical Concept Nodes"
      ],
      "metadata": {
        "id": "1GWvbSVqa3E6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in medical_concepts:\n",
        "  cypher_query_codisep.append(\"(\"+keep_only_english_characters(i.lower())+\":medical_concept { name: \"+'\"'+i+'\"'+\" }),\")"
      ],
      "metadata": {
        "id": "FEYKEkpPauI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Medical Synonyms"
      ],
      "metadata": {
        "id": "fD9Db8pJbWXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in medical_synonyms:\n",
        "  cypher_query_codisep.append(\"(\"+keep_only_english_characters(i.lower())+\":medical_synonym { name: \"+'\"'+i+'\"'+\" }),\")"
      ],
      "metadata": {
        "id": "ItNY7Fo_bN-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Medical Definitions"
      ],
      "metadata": {
        "id": "o77yQlKhbcGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in medical_definitions:\n",
        "  cypher_query_codisep.append(\"(\"+keep_only_english_characters(i.lower())+\":medical_definition { name: \"+'\"'+i+'\"'+\" }),\")"
      ],
      "metadata": {
        "id": "zthjmOvWbfQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cypher_query_codisep"
      ],
      "metadata": {
        "id": "_IDHacO1Ts4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for line in cypher_query_codisep:\n",
        "  with open(\"cypher_query.txt\", \"a\") as query:\n",
        "    query.write(line)"
      ],
      "metadata": {
        "id": "lvbP-ZAdcTOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Taking Data from user. (Structured Data)"
      ],
      "metadata": {
        "id": "9l5g5221G0Pe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def take_data():\n",
        " concept = input(\"Add desired medical concept: \")\n",
        " check = int(input(\"Press 1 to add another and 0 to abort: \"))  # Convert input to integer\n",
        " user_defined_concepts.append(concept)\n",
        " try:\n",
        "  if check==1:\n",
        "   take_data()\n",
        "  else:\n",
        "   pass\n",
        " except:\n",
        "   pass"
      ],
      "metadata": {
        "id": "N_lNYW9GjqBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get user data at runtime with type conversion\n",
        "user_defined_concepts=[]\n",
        "take_data()\n",
        "\n",
        "user_defined_unique_concepts = list(set(user_defined_concepts))\n",
        "\n",
        "# Display the collected data\n",
        "print(\"User Entered Concepts:\", user_defined_unique_concepts)\n",
        "\n"
      ],
      "metadata": {
        "id": "vbD756-jGzzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bioportal Section"
      ],
      "metadata": {
        "id": "NhfhppUqM3db"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CPOL4jfGHKs"
      },
      "outputs": [],
      "source": [
        "import urllib.request, urllib.error, urllib.parse\n",
        "import json\n",
        "import os\n",
        "from pprint import pprint\n",
        "REST_URL = \"http://data.bioontology.org\"\n",
        "API_KEY = \"396993d0-4ce2-4123-93de-214e9b9ebcf2\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_json(url):\n",
        "    opener = urllib.request.build_opener()\n",
        "    opener.addheaders = [('Authorization', 'apikey token=' + API_KEY)]\n",
        "    return json.loads(opener.open(url).read())"
      ],
      "metadata": {
        "id": "1vefoUGNNN0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions For Knowledge Filteration\n",
        "1. Mainly used for Knowledge Graph Nodes Key Generation"
      ],
      "metadata": {
        "id": "lSK242oExz_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def keep_only_english_characters(input_string):\n",
        "    # Remove numbers and special characters\n",
        "    cleaned_string = re.sub(r'[^a-zA-Z]', '', input_string)\n",
        "\n",
        "    # Remove non-English characters\n",
        "    cleaned_string = re.sub(r'[^\\x00-\\x7F]', '', cleaned_string)\n",
        "\n",
        "    return cleaned_string.lower()\n",
        "\n",
        "def contains_english_character(input_string):\n",
        "    for char in input_string:\n",
        "        ascii_value = ord(char)\n",
        "        if 65 <= ascii_value <= 122:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def remove_quotes(input_string):\n",
        "    cleaned_string = input_string.replace('\"', '').replace(\"'\", '')\n",
        "    return cleaned_string\n",
        "\n",
        "# Example usage\n",
        "original_string = 'This is a \"quoted\" string with some \\'quotes\\'.'\n",
        "cleaned_string = remove_quotes(original_string)\n",
        "print(cleaned_string)\n"
      ],
      "metadata": {
        "id": "XeBXVf0VNABI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keep_only_english_characters(\"fig 18 abbr: ta-i1,ta-i4,etc. in isbn:0-937548-00-6.\")"
      ],
      "metadata": {
        "id": "yy8tyr_Wfdf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fuzzy Matching to remove duplicates"
      ],
      "metadata": {
        "id": "bJN0ibhmyLg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fuzzywuzzy"
      ],
      "metadata": {
        "id": "6qg5oa7rYQUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fuzzywuzzy import fuzz\n",
        "from fuzzywuzzy import process\n",
        "\n",
        "#defining threshold\n",
        "similarity_threshold = 80\n",
        "\n",
        "def remove_duplicates(strings):\n",
        "    duplicates = set()\n",
        "    unique_strings = []\n",
        "\n",
        "    for string1 in strings:\n",
        "        is_duplicate = False\n",
        "        for string2 in unique_strings:\n",
        "            similarity_score = fuzz.token_sort_ratio(string1, string2)\n",
        "            if similarity_score >= similarity_threshold:\n",
        "                is_duplicate = True\n",
        "                duplicates.add(string2)\n",
        "                break\n",
        "        if not is_duplicate:\n",
        "            unique_strings.append(string1)\n",
        "\n",
        "    return unique_strings, duplicates\n"
      ],
      "metadata": {
        "id": "jc4-cYSGX5ib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "    \"This is a sample sentence.\",\n",
        "    \"This is a sample sentence\",\n",
        "    \"Another example sentence.\",\n",
        "    \"Yet another example.\",\n",
        "    \"This is a similar sentence.\",\n",
        "]\n",
        "\n",
        "unique_sentences, duplicates = remove_duplicates(sentences)\n",
        "\n",
        "print(\"Unique Sentences:\")\n",
        "for sentence in unique_sentences:\n",
        "    print(sentence)\n",
        "\n",
        "print(\"\\nDuplicates:\")\n",
        "for duplicate in duplicates:\n",
        "    print(duplicate)\n"
      ],
      "metadata": {
        "id": "oZuTNxcpYHUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bioportal Synonyms and Definitions retrieval function"
      ],
      "metadata": {
        "id": "mgrJZ3iKydqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_synonyms(concept):\n",
        "  search_results,synonym,definition = [],[],[]\n",
        "  syno_fin=\"\"\n",
        "  search_results.append(get_json(REST_URL + \"/search?q=\" + concept)[\"collection\"])\n",
        "  #collecting enriched knowledge and adding that in respective lists.\n",
        "  for result in search_results:\n",
        "    for i in range(len(result)):\n",
        "     data=result[i]\n",
        "     for k in dict.keys(data):\n",
        "       if k==\"synonym\":\n",
        "         for i in data[\"synonym\"]:\n",
        "           synonym.append(i)\n",
        "       if k==\"definition\":\n",
        "         for i in data[\"definition\"]:\n",
        "           definition.append(i)\n",
        "\n",
        "  #removing duplicates synonyms and creating unique list\n",
        "  syn_new=[]\n",
        "\n",
        "  for i in synonym:\n",
        "   syn_new.append(i.lower())\n",
        "\n",
        "  myset = set(syn_new)\n",
        "  syno_new=list(myset)\n",
        "  #print(syno_new)\n",
        "  syno_new = list(dict.fromkeys(syno_new))\n",
        "  unique_synonyms, duplicates = remove_duplicates(syno_new)\n",
        "  #adding definition nodes and their relationships in graphs\n",
        "  for i in unique_synonyms:\n",
        "    if contains_english_character(i)==True:\n",
        "     print(i)\n",
        "     id=keep_only_english_characters(i)\n",
        "     syno_fin=syno_fin+i+\"@@@@@\"\n",
        "    else:\n",
        "      pass\n",
        "  return syno_fin\n"
      ],
      "metadata": {
        "id": "2Mm3vQYwNHTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_definitions(concept):\n",
        "  search_results,synonym,definition = [],[],[]\n",
        "  def_fin=\"\"\n",
        "  search_results.append(get_json(REST_URL + \"/search?q=\" + concept)[\"collection\"])\n",
        "  #collecting enriched knowledge and adding that in respective lists.\n",
        "  for result in search_results:\n",
        "    for i in range(len(result)):\n",
        "     data=result[i]\n",
        "     for k in dict.keys(data):\n",
        "       if k==\"synonym\":\n",
        "         for i in data[\"synonym\"]:\n",
        "           synonym.append(i)\n",
        "       if k==\"definition\":\n",
        "         for i in data[\"definition\"]:\n",
        "           definition.append(i)\n",
        "\n",
        "  #removing duplicates definitions and creating unique list\n",
        "  definintion_unique,new_define=[],[]\n",
        "  for i in definition:\n",
        "   new_define.append(i.lower())\n",
        "\n",
        "  defset=set(new_define)\n",
        "  definintion_unique=list(defset)\n",
        "  #print(definintion_unique)\n",
        "\n",
        "  #adding definition nodes and their relationships in graphs\n",
        "  if definintion_unique!=\"\":\n",
        "   definintion_unique = list(dict.fromkeys(definintion_unique))\n",
        "\n",
        "  unique_sentences, duplicates = remove_duplicates(definintion_unique)\n",
        "  for i in unique_sentences:\n",
        "   if contains_english_character(i)==True:\n",
        "    id=keep_only_english_characters(i)\n",
        "    def_fin=def_fin+i+\"@@@@@\"\n",
        "   else:\n",
        "    pass\n",
        "  return def_fin\n",
        "\n"
      ],
      "metadata": {
        "id": "4gvD3hLzY7bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Knowledge Graph or Cypher Query Creation"
      ],
      "metadata": {
        "id": "g-fn-OyEyrPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.parse import quote\n",
        "\n",
        "data_sorted=[]\n",
        "data_sorted_file_insertion=[]\n",
        "\n",
        "for i in user_defined_unique_concepts:\n",
        " data_sorted.append(i)\n",
        " encoded_query = quote(i)\n",
        " data_sorted.append(get_definitions(encoded_query))\n",
        " data_sorted.append(get_synonyms(encoded_query))\n",
        " data_sorted_file_insertion.append(data_sorted)\n",
        " data_sorted=[]\n",
        "\n",
        "print(data_sorted_file_insertion)"
      ],
      "metadata": {
        "id": "gFHYWrHCoZvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TSV file used as a backup/ for future purpose"
      ],
      "metadata": {
        "id": "DNkf24_1ySlx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "\n",
        "# Path for the new TSV file\n",
        "tsv_file_path = 'graph_data_sorted.tsv'\n",
        "\n",
        "# Write data to the TSV file\n",
        "with open(tsv_file_path, 'w', newline='', encoding='utf-8') as tsvfile:\n",
        "    writer = csv.writer(tsvfile, delimiter='\\t')\n",
        "    writer.writerows(data_sorted_file_insertion)\n",
        "\n",
        "print(f\"New TSV file '{tsv_file_path}' has been created.\")"
      ],
      "metadata": {
        "id": "dzjPi80Phv-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cypher_query=[]\n",
        "cypher_query.append(\"create \")\n",
        "f= open(\"cypher_query.txt\",\"w+\")\n",
        "f.close()\n",
        "with open(\"cypher_query.txt\", \"a\") as query:\n",
        "  query.write(\"Create\")"
      ],
      "metadata": {
        "id": "5TLxW75MnhHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i in data_sorted_file_insertion:\n",
        "  x=0\n",
        "  for n in i:\n",
        "    if x==0:\n",
        "      cypher_query.append(\"(\"+n.lower()+\"concept:medical_concept { name: \"+'\"'+remove_quotes(n)+'\"'+\" }),\")\n",
        "    # for medical definitions\n",
        "    elif x==1:\n",
        "      # Split the string using @@@@@ as the delimiter\n",
        "      split_list = n.split(\"@@@@@\")\n",
        "      for m in split_list:\n",
        "        cypher_query.append(\"(\"+keep_only_english_characters(m.lower())+\":definition { name: \"+'\"'+remove_quotes(m)+'\"'+\" }),\")\n",
        "        cypher_query.append(\"(\"+keep_only_english_characters(m.lower())+\")-[:Definition]->(\"+i[0].lower()+\"concept),\")\n",
        "\n",
        "    # for medical synonyms\n",
        "    elif x==2:\n",
        "      # Split the string using @@@@@ as the delimiter\n",
        "      split_list = n.split(\"@@@@@\")\n",
        "      for m in split_list:\n",
        "        cypher_query.append(\"(\"+keep_only_english_characters(m.lower())+\":synonyms { name: \"+'\"'+remove_quotes(m)+'\"'+\" }),\")\n",
        "        cypher_query.append(\"(\"+keep_only_english_characters(m.lower())+\")-[:Synonym]->(\"+i[0].lower()+\"concept),\")\n",
        "    x=x+1\n"
      ],
      "metadata": {
        "id": "ziL4rL5Vk0Pw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Remove ending comma from the last string within the list cypher_query\n",
        "#remove last comma from the ending string from within the list cypher_query\n",
        "print(cypher_query[len(cypher_query)-1])\n",
        "lastString=cypher_query[len(cypher_query)-1]\n",
        "length=len(lastString)\n",
        "trimmedLastString=lastString[:length-1]\n",
        "print(trimmedLastString)\n",
        "cypher_query[len(cypher_query)-1]=trimmedLastString"
      ],
      "metadata": {
        "id": "SkTJI1KfZlfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text File creation"
      ],
      "metadata": {
        "id": "kpIB0lHry50M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Specify the file path\n",
        "file_path = 'cypher_query.txt'\n",
        "\n",
        "# Open the file for writing\n",
        "with open(file_path, 'w') as file:\n",
        "    # Write each item to a separate line in the file\n",
        "    for item in cypher_query:\n",
        "        file.write(item)\n",
        "\n",
        "print(f\"Items written to {file_path}\")\n"
      ],
      "metadata": {
        "id": "86N4yqIRnCYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# README\n",
        "\n",
        "\n",
        "\n",
        "------------------------------------------------------\n",
        "# New Section\n",
        "1. Now need to work on duplicate nodes and have to create relationships-->(Done--99% automated, 1% chance of erros incase if generic terms are used to create KG)--> Can add instructions.\n",
        "\n",
        "2. For next week, need to create connections among concepts (use Neo4j algorithms or use word embeddings)"
      ],
      "metadata": {
        "id": "7f5KGKlfu5iJ"
      }
    }
  ]
}